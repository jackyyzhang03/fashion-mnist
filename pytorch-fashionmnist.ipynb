{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport time\nimport copy","metadata":{"execution":{"iopub.status.busy":"2022-04-02T12:43:16.50107Z","iopub.execute_input":"2022-04-02T12:43:16.501362Z","iopub.status.idle":"2022-04-02T12:43:16.508106Z","shell.execute_reply.started":"2022-04-02T12:43:16.50133Z","shell.execute_reply":"2022-04-02T12:43:16.506393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>1. Load training images</h3>","metadata":{}},{"cell_type":"code","source":"train_images = torch.tensor(np.load('../input/mais-202-winter-2022/train_images.npy').astype('uint8'))\ndf = pd.read_csv('../input/mais-202-winter-2022/train_labels.csv')\nlabels = torch.tensor(df['label'])","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:18:34.2711Z","iopub.execute_input":"2022-04-02T01:18:34.271402Z","iopub.status.idle":"2022-04-02T01:18:34.596537Z","shell.execute_reply.started":"2022-04-02T01:18:34.271375Z","shell.execute_reply":"2022-04-02T01:18:34.595802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>2. Visualize images </h3>","metadata":{}},{"cell_type":"code","source":"label_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:18:34.598339Z","iopub.execute_input":"2022-04-02T01:18:34.598618Z","iopub.status.idle":"2022-04-02T01:18:34.603245Z","shell.execute_reply.started":"2022-04-02T01:18:34.598581Z","shell.execute_reply":"2022-04-02T01:18:34.602372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image(arr, title):\n    two_d = np.reshape(arr, (28, 28)) * 255\n    plt.imshow(two_d, interpolation='nearest', cmap='gray')\n    plt.title(label_names[title])\n    plt.show()\n\nfor i in range(5):\n    show_image(train_images[i], labels[i])","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:18:34.604668Z","iopub.execute_input":"2022-04-02T01:18:34.604938Z","iopub.status.idle":"2022-04-02T01:18:35.539Z","shell.execute_reply.started":"2022-04-02T01:18:34.604898Z","shell.execute_reply":"2022-04-02T01:18:35.538306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>3. Define Dataset and Dataloaders</h3>","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.RandomCrop(28, padding=2),\n    transforms.RandomHorizontalFlip(),\n    transforms.ConvertImageDtype(torch.float),\n    transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081)),\n    transforms.RandomErasing(p=0.2)\n])\n\ntest_transforms = transforms.Compose([\n    transforms.ConvertImageDtype(torch.float),\n    transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081))\n])","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:12:05.388887Z","iopub.execute_input":"2022-04-02T13:12:05.389237Z","iopub.status.idle":"2022-04-02T13:12:05.398423Z","shell.execute_reply.started":"2022-04-02T13:12:05.389204Z","shell.execute_reply":"2022-04-02T13:12:05.397269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport cv2\n\nclass FashionMNIST(Dataset):\n    def __init__(self, images, labels=None, transforms=None):\n        self.labels = labels\n        self.transforms = transforms;\n        self.images = torch.full((images.shape[0], 3, 28, 28), 0.0)\n        for index, image in enumerate(images):\n            self.images[index] = torch.Tensor(np.moveaxis(cv2.cvtColor(image.numpy(), cv2.COLOR_GRAY2RGB), -1, 0))\n            \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, index):\n        image = self.images[index]\n        if self.transforms is not None:\n            image = self.transforms(image)\n        if self.labels is not None:\n            label = self.labels[index]\n            return (image, label)\n        return image","metadata":{"execution":{"iopub.status.busy":"2022-04-02T12:43:22.311107Z","iopub.execute_input":"2022-04-02T12:43:22.311677Z","iopub.status.idle":"2022-04-02T12:43:22.761892Z","shell.execute_reply.started":"2022-04-02T12:43:22.311634Z","shell.execute_reply":"2022-04-02T12:43:22.760788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_images, labels, test_size=0.2)\n\ntrain_ds = FashionMNIST(X_train, y_train, transforms=train_transforms)\nvalid_ds = FashionMNIST(X_valid, y_valid, transforms=test_transforms)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:21:38.510714Z","iopub.execute_input":"2022-04-02T01:21:38.510971Z","iopub.status.idle":"2022-04-02T01:21:40.319469Z","shell.execute_reply.started":"2022-04-02T01:21:38.510942Z","shell.execute_reply":"2022-04-02T01:21:40.318679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\nvalid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:21:40.320932Z","iopub.execute_input":"2022-04-02T01:21:40.321172Z","iopub.status.idle":"2022-04-02T01:21:40.326325Z","shell.execute_reply.started":"2022-04-02T01:21:40.321138Z","shell.execute_reply":"2022-04-02T01:21:40.32569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_datasets = {\n    'train': train_ds,\n    'val': valid_ds\n}\n\ndataloaders = {\n    'train': train_dl,\n    'val': valid_dl\n}\n\ndataset_sizes = {\n    'train': len(train_ds),\n    'val': len(valid_ds)\n}","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:21:40.327463Z","iopub.execute_input":"2022-04-02T01:21:40.328471Z","iopub.status.idle":"2022-04-02T01:21:40.340275Z","shell.execute_reply.started":"2022-04-02T01:21:40.328417Z","shell.execute_reply":"2022-04-02T01:21:40.339459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>4. Define Model</h3>","metadata":{}},{"cell_type":"code","source":"pip install efficientnet_pytorch","metadata":{"execution":{"iopub.status.busy":"2022-04-02T12:43:38.075768Z","iopub.execute_input":"2022-04-02T12:43:38.076182Z","iopub.status.idle":"2022-04-02T12:43:51.830874Z","shell.execute_reply.started":"2022-04-02T12:43:38.076109Z","shell.execute_reply":"2022-04-02T12:43:51.829588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_pretrained('efficientnet-b7', num_classes=10)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T12:43:51.83374Z","iopub.execute_input":"2022-04-02T12:43:51.834438Z","iopub.status.idle":"2022-04-02T12:44:15.324351Z","shell.execute_reply.started":"2022-04-02T12:43:51.834396Z","shell.execute_reply":"2022-04-02T12:44:15.323361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T12:44:29.435264Z","iopub.execute_input":"2022-04-02T12:44:29.435694Z","iopub.status.idle":"2022-04-02T12:44:29.551248Z","shell.execute_reply.started":"2022-04-02T12:44:29.435633Z","shell.execute_reply":"2022-04-02T12:44:29.550125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:21:49.830941Z","iopub.execute_input":"2022-04-02T01:21:49.831192Z","iopub.status.idle":"2022-04-02T01:21:49.840607Z","shell.execute_reply.started":"2022-04-02T01:21:49.831157Z","shell.execute_reply":"2022-04-02T01:21:49.839513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n    best_weights = copy.deepcopy(model.state_dict())\n    best_accuracy = 0.0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch + 1}/{num_epochs}')\n        print('-' * 10)\n        \n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0\n            \n            for images, labels in dataloaders[phase]:\n                images = images.to(device)\n                labels = labels.to(device)\n                \n                optimizer.zero_grad()\n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(images)\n                    loss = criterion(outputs, labels)\n                    _, preds = torch.max(outputs, 1)\n                    \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                running_loss += loss.item() * images.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_accuracy = running_corrects.double() / dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Accuracy: {:.4f}'.format(phase.capitalize(), epoch_loss, epoch_accuracy))\n\n            if phase == 'val' and epoch_accuracy > best_accuracy:\n                best_accuracy = epoch_accuracy\n                best_weights = copy.deepcopy(model.state_dict())\n                \n        scheduler.step()\n        print()\n    \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best validation accuracy: {:4f}'.format(best_accuracy))\n\n    model.load_state_dict(best_weights)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:21:49.84217Z","iopub.execute_input":"2022-04-02T01:21:49.842444Z","iopub.status.idle":"2022-04-02T01:21:49.856719Z","shell.execute_reply.started":"2022-04-02T01:21:49.842408Z","shell.execute_reply":"2022-04-02T01:21:49.856015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>5. Train Model</h3>","metadata":{}},{"cell_type":"code","source":"model = train(model, criterion, optimizer, scheduler, num_epochs=100)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T01:21:49.859608Z","iopub.execute_input":"2022-04-02T01:21:49.860196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'state.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>6. Inference</h3>","metadata":{}},{"cell_type":"code","source":"test = torch.tensor(np.load('../input/mais-202-winter-2022/test_images.npy').astype('uint8'))","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:12:23.64568Z","iopub.execute_input":"2022-04-02T13:12:23.646028Z","iopub.status.idle":"2022-04-02T13:12:23.715811Z","shell.execute_reply.started":"2022-04-02T13:12:23.645992Z","shell.execute_reply":"2022-04-02T13:12:23.714825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = FashionMNIST(test, transforms=test_transforms)\ntest_dl = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:12:24.809874Z","iopub.execute_input":"2022-04-02T13:12:24.810479Z","iopub.status.idle":"2022-04-02T13:12:25.973707Z","shell.execute_reply.started":"2022-04-02T13:12:24.810442Z","shell.execute_reply":"2022-04-02T13:12:25.972699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict():\n    model.eval()\n    y_pred = None\n    for i, images in enumerate(test_dl):\n        with torch.no_grad():\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            if (y_pred is None):\n                y_pred = preds\n            else:\n                y_pred = torch.cat((y_pred, preds))\n    return y_pred.cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:12:27.095279Z","iopub.execute_input":"2022-04-02T13:12:27.095931Z","iopub.status.idle":"2022-04-02T13:12:27.103654Z","shell.execute_reply.started":"2022-04-02T13:12:27.09588Z","shell.execute_reply":"2022-04-02T13:12:27.102122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = predict()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:12:28.991774Z","iopub.execute_input":"2022-04-02T13:12:28.992122Z","iopub.status.idle":"2022-04-02T13:12:51.916704Z","shell.execute_reply.started":"2022-04-02T13:12:28.99209Z","shell.execute_reply":"2022-04-02T13:12:51.913953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('../input/mais-202-winter-2022/sample_submission.csv')\ndf['label'] = y_pred\ndf.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:12:51.918985Z","iopub.execute_input":"2022-04-02T13:12:51.919565Z","iopub.status.idle":"2022-04-02T13:12:52.002461Z","shell.execute_reply.started":"2022-04-02T13:12:51.919511Z","shell.execute_reply":"2022-04-02T13:12:52.001216Z"},"trusted":true},"execution_count":null,"outputs":[]}]}